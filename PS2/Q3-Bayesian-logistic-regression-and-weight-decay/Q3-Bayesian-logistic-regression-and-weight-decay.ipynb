{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Proposition__ We consider a logistic regression model $h_{\\theta}(x)=g(\\theta^Tx)$ where $g$ is the sigmoid function. Define\n",
    "\n",
    "$$ \\theta_{ML}\\equiv\\mathop{\\arg \\max}_{\\theta} \\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}; \\theta)\\tag{1} $$\n",
    "\n",
    "$$ \\theta_{MAP}\\equiv\\mathop{\\arg \\max}_{\\theta} p(\\theta) \\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)\\tag{2}$$\n",
    "\n",
    "and suppose that\n",
    "\n",
    "$$\n",
    "\\Theta \\sim \\mathcal{N}(0, \\tau^{2}I)\n",
    "$$\n",
    "\n",
    "where, $\\tau > 0$, and $I$ is the $(n + 1) \\times (n + 1)$ identity matrix. Then\n",
    "\n",
    "$$\\lVert\\theta_{MAP}\\rVert_2 \\le \\lVert\\theta_{ML}\\rVert_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Proof by Contradiction__ Suppose that\n",
    "\n",
    "$$\\lVert\\theta_{MAP}\\rVert_2 \\gt \\left|\\left|\\theta_{ML}\\right|\\right|_2\\tag{3}$$\n",
    "\n",
    "Let's first look at the univariate case ($\\theta\\in\\mathbb{R}$). The computed densities are\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta_{MAP}) = \\frac{1}{\\sqrt{2\\pi}\\tau}e^{-\\frac{(\\theta_{MAP}-0)^2}{2\\tau^2}} = \\frac{1}{\\sqrt{2\\pi}\\tau}e^{-\\frac{\\theta_{MAP}^2}{2\\tau^2}} \\lt \\frac{1}{\\sqrt{2\\pi}\\tau}e^{-\\frac{\\theta_{ML}^2}{2\\tau^2}}  = p(\\theta_{ML})\n",
    "\\end{align*}$$\n",
    "\n",
    "The inequality holds because our supposition in (3) gives\n",
    "\n",
    "$$\\theta_{MAP}^2=\\big(\\sqrt{\\theta_{MAP}^2}\\big)^2=\\lVert\\theta_{MAP}\\rVert_2^2 \\gt \\lVert\\theta_{ML}\\rVert_2^2=\\big(\\sqrt{\\theta_{ML}^2}\\big)^2=\\theta_{ML}^2$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\begin{align*}\n",
    "-\\frac{\\theta_{MAP}^2}{2\\tau^2} \\lt -\\frac{\\theta_{ML}^2}{2\\tau^2}\n",
    "\\end{align*}$$\n",
    "\n",
    "and the strictly increasing monotonicity of the exponential function gives\n",
    "\n",
    "$$\\begin{align*}\n",
    "e^{-\\frac{\\theta_{MAP}^2}{2\\tau^2}} \\lt e^{-\\frac{\\theta_{ML}^2}{2\\tau^2}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly, for the multivariate case, we have\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta) &= \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\lvert\\tau^2I\\rvert^{\\frac{1}{2}}}\\exp\\Big(-\\frac{1}{2}(\\theta-0)^T(\\tau^2I)^{-1}(\\theta-0)\\Big)\\\\\n",
    "    &= \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}(\\tau^2)^{\\frac{1}{2}}\\lvert I\\rvert^{\\frac{1}{2}}}\\exp\\Big(-\\frac{1}{2}\\theta^TI^{-1}(\\tau^2)^{-1}\\theta\\Big)\\\\\n",
    "    &= \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau1^{\\frac{1}{2}}}\\exp\\Big(-\\frac{1}{2}\\theta^TI\\frac{1}{\\tau^2}\\theta\\Big)\\\\\n",
    "    &= \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau}\\exp\\Big(-\\frac{1}{2\\tau^2}\\theta^T\\theta\\Big)\\\\\n",
    "    &= \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau}\\exp\\Big(-\\frac{\\lVert\\theta\\rVert_2^2}{2\\tau^2}\\Big)\n",
    "\\end{align*}$$\n",
    "\n",
    "The supposition in (3) again gives\n",
    "\n",
    "$$\\begin{align*}\n",
    "-\\frac{\\lVert\\theta_{MAP}\\rVert_2^2}{2\\tau^2} \\lt -\\frac{\\lVert\\theta_{ML}\\rVert_2^2}{2\\tau^2}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta_{MAP}) = \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau}\\exp\\Big(-\\frac{\\lVert\\theta_{MAP}\\rVert_2^2}{2\\tau^2}\\Big) \\lt \\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau}\\exp\\Big(-\\frac{\\lVert\\theta_{ML}\\rVert_2^2}{2\\tau^2}\\Big) = p(\\theta_{ML})\n",
    "\\end{align*}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta_{MAP}) \\prod_{i=1}^{m}  p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta_{MAP}) &= p(\\theta_{MAP}) \\prod_{i=1}^{m}  p(y^{(i)}\\mid x^{(i)}; \\theta_{MAP}) \\\\\n",
    "    &<  p(\\theta_{ML}) \\prod_{i=1}^{m}  p(y^{(i)}\\mid x^{(i)}; \\theta_{MAP}) \\\\\n",
    "    &< p(\\theta_{ML}) \\prod_{i=1}^{m}  p(y^{(i)}\\mid x^{(i)}; \\theta_{ML}) \\\\\n",
    "    &= p(\\theta_{ML}) \\prod_{i=1}^{m}  p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta_{ML}) \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "The first and last equalities follow because, under the probabilistic interpretation (see p.4-6 of loss_functions.pdf) of logistic regression, we have\n",
    "\n",
    "$$\n",
    "p(y^{(i)}\\mid x^{(i)}; \\theta) = g(y^{(i)}\\theta^Tx^{(i)}) = p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)\n",
    "$$\n",
    "\n",
    "The second inequality holds because, by definition (1), $\\theta_{ML}$ maximizes $\\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}; \\theta)$.\n",
    "\n",
    "But this contradicts that $\\theta_{MAP}$ maximizes $p(\\theta)\\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)$ from definition (2). Hence the supposition in (3) is wrong.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remark__ \"For this reason, this form of regularization is sometimes also called weight decay, since it encourages the weights (meaning parameters) to take on generally smaller values.\"\n",
    "\n",
    "Note that, for the MAP estimation, the assumption of a Gaussian prior on $\\theta$ is equivalent to the ML estimation with L2 regularization:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta_{ML} &\\equiv \\mathop{\\arg \\max}_{\\theta} \\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}; \\theta) \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta} \\log\\big[\\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}; \\theta)\\big] \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Big\\{\\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}; \\theta)]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\Big\\{-\\sum_{i=1}^{m} \\log[g(y^{(i)}\\theta^Tx^{(i)})]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\Big\\{-\\sum_{i=1}^{m} \\log\\big[\\frac{1}{1+e^{-y^{(i)}\\theta^Tx^{(i)}}}\\big]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\Big\\{-\\sum_{i=1}^{m} \\log\\Big[\\big(1+e^{-y^{(i)}\\theta^Tx^{(i)}}\\big)^{-1}\\Big]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\sum_{i=1}^{m} \\log\\big(1+e^{-y^{(i)}\\theta^Tx^{(i)}}\\big) \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}J(\\theta) \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $J(\\theta)\\equiv\\sum_{i=1}^{m} \\log\\big(1+e^{-y^{(i)}\\theta^Tx^{(i)}}\\big)$ is the logistic objective that was detailed in PS2-Q1-Logistic-Regression-Training-Stability. And\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta_{MAP} &\\equiv \\mathop{\\arg \\max}_{\\theta} p(\\theta) \\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta) \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta} \\log\\big[p(\\theta) \\prod_{i=1}^{m} p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)\\big] \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Big\\{\\log[p(\\theta)] + \\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Bigg\\{\\log\\Bigg[\\frac{1}{(2\\pi)^{\\frac{n+1}{2}}\\tau}\\exp\\Big(-\\frac{\\lVert\\theta\\rVert_2^2}{2\\tau^2}\\Big)\\Bigg] + \\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)]\\Bigg\\} \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Bigg\\{\\log\\Bigg[\\exp\\Big(-\\frac{\\lVert\\theta\\rVert_2^2}{2\\tau^2}\\Big)\\Bigg] + \\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)]\\Bigg\\} \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Big\\{-\\frac{\\lVert\\theta\\rVert_2^2}{2\\tau^2} + \\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\max}_{\\theta}\\Big\\{-\\lVert\\theta\\rVert_2^2 + \\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)]\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\Big\\{-\\sum_{i=1}^{m} \\log[p(y^{(i)}\\mid x^{(i)}, \\Theta=\\theta)] + \\lVert\\theta\\rVert_2^2\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\Big\\{\\sum_{i=1}^{m} \\log\\big(1+e^{-y^{(i)}\\theta^Tx^{(i)}}\\big) + \\lVert\\theta\\rVert_2^2\\Big\\} \\\\\n",
    "    &= \\mathop{\\arg \\min}_{\\theta}\\big(J(\\theta) + \\lVert\\theta\\rVert_2^2\\big) \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "I wonder if it's possible to show that L2 regularization adds statistical shrinkage without resorting to the contradiction proof above?\n",
    "\n",
    "We computed the gradient of $J(\\theta)$ in PS2-Q1-Logistic-Regression-Training-Stability, Equation LLG.1:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla J(\\theta) &= -\\sum_{i=1}^{m}g\\big(-y^{(i)}\\theta^Tx^{(i)}\\big)y^{(i)}x^{(i)}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "The gradient of the L2 regularization term is\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_\\theta(\\lVert\\theta\\rVert_2^2)=\\nabla_\\theta(\\theta^T\\theta)=\\nabla_\\theta(\\theta^TI\\theta)=2I\\theta=2\\theta\n",
    "\\end{align*}$$\n",
    "\n",
    "The next-to-last equality follows from PS0-Q1a.\n",
    "\n",
    "Setting the gradients to zero, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 = -\\sum_{i=1}^{m}g\\big(-y^{(i)}\\theta_{ML}^Tx^{(i)}\\big)y^{(i)}x^{(i)}\n",
    "\\end{align*}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 = \\sum_{i=1}^{m}g\\big(-y^{(i)}\\theta_{ML}^Tx^{(i)}\\big)y^{(i)}x^{(i)}\\tag{4}\n",
    "\\end{align*}$$\n",
    "\n",
    "For the MAP estimate, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 = -\\sum_{i=1}^{m}g\\big(-y^{(i)}\\theta_{MAP}^Tx^{(i)}\\big)y^{(i)}x^{(i)} + 2\\theta_{MAP}\n",
    "\\end{align*}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 = \\sum_{i=1}^{m}g\\big(-y^{(i)}\\theta_{MAP}^Tx^{(i)}\\big)y^{(i)}x^{(i)} - 2\\theta_{MAP}\\tag{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "So is it possible to use (4) and (5) to show that $\\lVert\\theta_{MAP}\\rVert_2 \\le \\lVert\\theta_{ML}\\rVert_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
